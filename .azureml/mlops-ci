# The script to run.
script:
# The arguments to the script file.
arguments: []
# The name of the compute target to use for this run.
target: mlops-ci
# Framework to execute inside. Allowed values are "Python" ,  "PySpark", "CNTK",  "TensorFlow", and "PyTorch".
framework: Python
# Communicator for the given framework. Allowed values are "None" ,  "ParameterServer", "OpenMpi", and "IntelMpi".
communicator: None
# Maximum allowed duration for the run.
maxRunDurationSeconds:
# Number of nodes to use for running job.
nodeCount: 1
# Environment details.
environment:
# Environment name
  name:
# Environment version
  version:
# Environment variables set for the run.
  environmentVariables:
    EXAMPLE_ENV_VAR: EXAMPLE_VALUE
# Python details
  python:
# user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.
    userManagedDependencies: false
# The python interpreter path
    interpreterPath: python
# Path to the conda dependencies file to use for this run. If a project
# contains multiple programs with different sets of dependencies, it may be
# convenient to manage those environments with separate files.
    condaDependenciesFile: .azureml/conda_dependencies.yml
# The base conda environment used for incremental environment creation.
    baseCondaEnvironment:
# Docker details
  docker:
# Set True to perform this run inside a Docker container.
    enabled: true
# Base image used for Docker-based runs. Mutually exclusive with base_dockerfile.
    baseImage: mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1
# Base Dockerfile used for Docker-based runs. Mutually exclusive with base_image
    baseDockerfile:
# Set False if necessary to work around shared volume bugs.
    sharedVolumes: true
# Shared memory size for Docker container. Default is 2g.
    shmSize: 2g
# Extra arguments to the Docker run command.
    arguments: []
# Image registry that contains the base image.
    baseImageRegistry:
# DNS name or IP address of azure container registry(ACR)
      address:
# The username for ACR
      username:
# The password for ACR
      password:
# RegistryIdentity
      registryIdentity:
# Docker image platform
    platform:
# Operating System
      os: Linux
# Architecture
      architecture: amd64
# Spark details
  spark:
# List of spark repositories.
    repositories: []
# The packages to use.
    packages: []
# Whether to precache the packages.
    precachePackages: true
# Databricks details
  databricks:
# List of maven libraries.
    mavenLibraries: []
# List of PyPi libraries
    pypiLibraries: []
# List of RCran libraries
    rcranLibraries: []
# List of JAR libraries
    jarLibraries: []
# List of Egg libraries
    eggLibraries: []
# R details
  r:
# Inferencing stack version
  inferencingStackVersion:
# History details.
history:
# Enable history tracking -- this allows status, logs, metrics, and outputs
# to be collected for a run.
  outputCollection: true
# Whether to take snapshots for history.
  snapshotProject: true
# Directories to sync with FileWatcher.
  directoriesToWatch:
  - logs
# Spark configuration details.
spark:
# The Spark configuration.
  configuration:
    spark.app.name: Azure ML Experiment
    spark.yarn.maxAppAttempts: 1
# HDI details.
hdi:
# Yarn deploy mode. Options are cluster and client.
  yarnDeployMode: cluster
# Tensorflow details.
tensorflow:
# The number of worker tasks.
  workerCount: 1
# The number of parameter server tasks.
  parameterServerCount: 1
# Mpi details.
mpi:
# When using MPI, number of processes per node.
  processCountPerNode: 1
# The number of nodes to use for the job.
  nodeCount: 1
# ParallelTask details.
paralleltask:
# Maximum number of retries per worker. Default 0.
  maxRetriesPerWorker: 0
# The number of workers/processes per node. Default 1.
  workerCountPerNode: 1
# Worker exit codes to terminate job.
  terminalExitCodes:
# data reference configuration details
dataReferences: {}
# The configuration that describes how to make data available for the run.
data: {}
# The configuration that describes how to save and track outputs for the run
outputData: {}
# Project share datastore reference.
sourceDirectoryDataStore:
# AmlCompute details.
amlcompute:
# VM size of the Cluster to be created.Allowed values are Azure vm sizes.The list of vm sizes is available in 'https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs
  vmSize:
# VM priority of the Cluster to be created. Allowed values are:"dedicated" , "lowpriority".
  vmPriority:
# A bool that indicates if the cluster has to be retained after job completion.
  retainCluster: false
# Name of the cluster to be created. If not specified, runId will be used as cluster name.
  name:
# Maximum number of nodes in the AmlCompute cluster to be created. Minimum number of nodes will always be set to 0.
  clusterMaxNodeCount:
# The command to be submitted for the run. The command property can also be used instead of script/arguments.
command: ''
